# -*- coding: utf-8 -*-
"""book-recomendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hFxJo2DB4naRtioJJXa0kbPvtqurtYWj

# **Book Recommnedation**
## **Content Based & Collaborative Filtering Recommnedation**
### *https://www.dicoding.com/users/lukiprasetyo*
Last Submission Task for "Machine Learning Developer - Machine Learning Terapan"

## Importing Dependencies
"""

import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from IPython.display import Image, HTML

"""## Load Dataset"""

!wget --no-check-certificate https://github.com/lukpras/datasets/raw/main/goodbooks-10k.zip

!unzip /content/goodbooks-10k.zip

book = pd.read_csv(r'/content/books.csv')
ratings = pd.read_csv(r'/content/ratings.csv')
to_read = pd.read_csv(r'/content/to_read.csv')

"""## Exploring the Data

### Book Data
"""

book.head()

book.info()

book.isnull().sum()

"""#### Cleaning book data"""

# Removing Duplicates
book.duplicated(
    subset =['original_title'],
    keep = False
).sum()

"""```
Therefore, 849 duplicates were present in the book dataset, that need to be removed.
```
"""

#Dropping null title
book.drop_duplicates(
    subset='original_title',
    keep=False,inplace=True
)
print(book.shape)

"""### Ratings"""

ratings.head()

ratings.info()

ratings.isnull().sum()

"""#### Cleaning ratings data"""

ratings=ratings.sort_values("user_id")
ratings.shape

# Find duplicates data
ratings.duplicated(
    subset =["user_id","book_id"],
    keep = False
).sum()

"""```
Therefore, 4487 duplicates were present in the ratings dataset, that need to be removed.
```
"""

# Removing duplicates data
ratings.drop_duplicates(
    subset =["user_id","book_id"],
    keep = False, inplace = True
) 
ratings.shape

"""## Data Visualization

function below essentially convert the image url to 
     '<img src="'+ path + '"/>' format. And one can put any
     formatting adjustments to control the height, aspect ratio, size etc.
     within as in the below example.
"""

def path_to_image_html(path):
    return '<img src="'+ path + '""/>'

"""### Top 10 Rated Books"""

# Top 10 Rated Books
top_rated = book.sort_values('average_rating', ascending=False)
top10 = top_rated.head(10)
f = ['title','small_image_url']
displ=(top10[f])
displ.set_index('title', inplace=True)
HTML(
    displ.to_html(
        escape=False,
        formatters=dict(small_image_url=path_to_image_html),
        justify='center'
    )
)

"""### Top 10 Most Popular Book"""

# Top 10 Most Popular Book
pop10=book.sort_values(by='ratings_count', ascending=False)
f=['title','small_image_url']
pop10=pop10.head(10)

pop10=(pop10[f])
pop10=pop10.set_index('title')

HTML(
    pop10.to_html(
        escape=False,
        formatters=dict(small_image_url=path_to_image_html),
        justify='center'
    )
)

"""### Most Common Rating Values"""

plt.figure(figsize=(16,8))
sns.distplot(a=book['average_rating'], kde=True, color='r')

"""```
Therefore, most common rating is somewhere between 3.5 to 4.
```

### Highly Rated Authors
"""

f=['authors', 'average_rating']
top_authors=top_rated[f]
top_authors=top_authors.head(20)

fig = px.bar(top_authors, x='authors', y='average_rating', color ='average_rating')
fig.show()

"""```
Above barplot shows the top rated authors. Bill Waterson is on the top with a rating of 4.82!
```

### Book that user want to read
"""

to_read = book.merge(
    to_read,
    left_on='book_id',
    right_on='book_id',
    how='inner'
)

to_read = to_read.groupby('original_title').count()

to_read = to_read.sort_values(by='id', ascending=False)
to_read20 = to_read.head(20)

fig = px.bar(to_read20, x=to_read20.index, y='id', color ='id')
fig.show()

"""# Recommendation System

## Content Based

### Data Preparation and Feature Engineering

#### Cleaning the data
"""

fillnabooks= book.fillna('')

# Making the data to lower case and removing whitespace
def clean_data(x):
        return str.lower(x.replace(" ", ""))

# extracting feature
features=['original_title','authors','average_rating']
fillednabooks = fillnabooks[features]
fillednabooks = fillednabooks.astype(str)
fillednabooks.dtypes

for feature in features:
    fillednabooks[feature] = fillednabooks[feature].apply(clean_data)
    
fillednabooks.head(2)

# Creating bag of words for all rows
def create_soup(x):
    return x['original_title']+ ' ' + x['authors'] + ' ' + x['average_rating']
fillednabooks['soup'] = fillednabooks.apply(create_soup, axis=1)

fillednabooks.head()

"""#### Transforming text to Vector"""

tfid = TfidfVectorizer()
tfid.fit(fillednabooks['soup'])
tfid.get_feature_names_out()

tfidf_matrix = tfid.fit_transform(fillednabooks['soup'])
tfidf_matrix.shape

tfidf_matrix

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfid.get_feature_names(),
    index=fillednabooks.soup
).sample(22, axis=1).sample(10, axis=0)

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=fillednabooks['original_title'], columns=fillednabooks['original_title'])
print('Shape:', cosine_sim_df.shape)
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### Get Recommendation"""

fillednabooks=fillednabooks.reset_index()
indices = pd.Series(fillednabooks.index, index=fillednabooks['original_title'])

def get_recommendations(title, cosine_sim=cosine_sim, k=11):
    title=title.replace(' ','').lower()
    idx = indices[title]

    # Get the pairwsie similarity scores of all books with that movie
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the books based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar books
    sim_scores = sim_scores[1:k]

    # Get the movie indices
    book_indices = [i[0] for i in sim_scores]

    # Return the top 10 most similar books
    return list(book['original_title'].iloc[book_indices])

"""#### Testing the Top 10 Recommendation"""

book_title = "The Hobbit"

content_recommend_list = get_recommendations(book_title, cosine_sim)
content_recommend = pd.DataFrame(content_recommend_list)
content_recommend.columns = ["Books Recommendation based on " + book_title]
content_recommend.index = np.arange(1,len(content_recommend)+1)
content_recommend

book_title = "Jurassic Park"

content_recommend_list = get_recommendations(book_title, cosine_sim)
content_recommend = pd.DataFrame(content_recommend_list)
content_recommend.columns = ["Books Recommendation based on " + book_title]
content_recommend.index = np.arange(1,len(content_recommend)+1)
content_recommend

"""## Collaborative Filtering

### Data Preparation and Feature Engineering

#### Cleaning the data
"""

# Drop null values
new_book = book[['book_id', 'original_title']].dropna()
new_book

# Count book rating based on user
ratings=ratings.dropna()
df_ratings_cnt_tmp = pd.DataFrame(ratings.groupby('rating').size(), columns=['count'])
df_ratings_cnt_tmp.head(10)

# Find books that not rated or rated 0
num_users = len(ratings.user_id.unique())
num_items = len(ratings.book_id.unique())
total_cnt = num_users * num_items
rating_zero_cnt = total_cnt - ratings.shape[0]

df_ratings_cnt = df_ratings_cnt_tmp.append(
    pd.DataFrame({'count': rating_zero_cnt}, index=[0.0]),
    verify_integrity=True,
).sort_index()
df_ratings_cnt

df_ratings_cnt['log_count'] = np.log(df_ratings_cnt['count'])
df_ratings_cnt

# Make visualization for counting rating in book
get_ipython().run_line_magic('matplotlib', 'inline')
ax = df_ratings_cnt[['count']].reset_index().rename(columns={'index': 'rating score'}).plot(
    x='rating score',
    y='count',
    kind='bar',
    figsize=(12, 8),
    title='Count for Each Rating Score (in Log Scale)',
    logy=True,
    fontsize=12,color='black'
)
ax.set_xlabel("book rating score")
ax.set_ylabel("number of ratings")

# calculate number of ratings on each book
df_books_cnt = pd.DataFrame(ratings.groupby('book_id').size(), columns=['count'])
df_books_cnt.head()

# now we need to take only books that have been rated atleast 50 times
# to get some idea of the reactions of users towards it.
popularity_t = 50
popular_books = list(set(df_books_cnt.query('count >= @popularity_t').index))
df_ratings_drop = ratings[ratings.book_id.isin(popular_books)]
print('shape of original ratings data: ', ratings.shape)
print('shape of ratings data after dropping unpopular book: ', df_ratings_drop.shape)

# get number of ratings given by every user
df_users_cnt = pd.DataFrame(df_ratings_drop.groupby('user_id').size(), columns=['count'])
df_users_cnt.head()

# Dropping users who have rated less than 50 times
ratings_t = 50
active_users = list(set(df_users_cnt.query('count >= @ratings_t').index))
df_ratings_drop_users = df_ratings_drop[df_ratings_drop.user_id.isin(active_users)]
print('shape of original ratings data: ', ratings.shape)
print('shape of ratings data after dropping both unpopular book and inactive users: ', df_ratings_drop_users.shape)

# Create new df after dropping unpopular book and inactive users
new_df = pd.merge(
    df_ratings_drop_users,
    new_book,
    left_on='book_id',
    right_on='book_id',
    how='inner')
new_df

# Change user id to list without the same values
user_ids = new_df['user_id'].unique().tolist()
print('list user_id: ', user_ids)
 
# Encoded the user id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user id : ', user_to_user_encoded)
 
# Encoded numbers to user id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded numbers to user id: ', user_encoded_to_user)

# Change book id to list without the same values
book_ids = new_df['book_id'].unique().tolist()
 
# Encoded the book id
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
 
# Encoded numbers to book id
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

# Mapping user to dataframe based on user_id
new_df['user'] = new_df['user_id'].map(user_to_user_encoded)
 
# Mapping book to dataframe based on book_id
new_df['book'] = new_df['book_id'].map(book_to_book_encoded)

new_df

# Find total user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Find total book
num_book = len(book_encoded_to_book)
print(num_book)
 
# Change rating to float
# new_df['rating'] = new_df['rating'].values.astype(np.float32)
 
# Find the minimum value in the rating
min_rating = min(new_df['rating'])
 
# Find the maximum value in the rating
max_rating = max(new_df['rating'])
 
print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

# Randomize the dataset
new_df = new_df.sample(frac=1, random_state=42)
new_df

"""#### Splitting data to train and validation sets"""

# Create variable to match book and user
x = new_df[['user', 'book']].values
 
# Create variable to make rating of the results 
y = new_df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Splitting dataset to 80% train and 20% validation
train_indices = int(0.8 * new_df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""### Creating Model"""

class RecommenderNet(tf.keras.Model):
 
  # Initialization
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # call layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # call layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # call layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # call layer embedding 4
 
    dot_user_book = tf.tensordot(user_vector, book_vector, 2) 
 
    x = dot_user_book + user_bias + book_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

"""### Compiling model"""

# Model Initializaation
model = RecommenderNet(num_users, num_book, 50)
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""#### Training model"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Testing the Top 10 Recommendation

#### Recommendation for User
"""

book_df = new_book
 
# Mengambil sample user
user_id = new_df.user_id.sample(1).iloc[0]
book_readed_by_user = new_df[new_df.user_id == user_id]
 
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
book_not_readed = book_df[~book_df['book_id'].isin(book_readed_by_user.book_id.values)]['book_id'] 
book_not_readed = list(
    set(book_not_readed)
    .intersection(set(book_to_book_encoded.keys()))
)
 
book_not_readed = [[book_to_book_encoded.get(x)] for x in book_not_readed]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_readed), book_not_readed)
)

ratings_predict_1 = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings_predict_1.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_readed[x][0]) for x in top_ratings_indices
]
 
print('\nShowing recommendations for users: {}\n'.format(user_id))
print('====' * 8)
print('Book with high ratings from user')
print('----' * 8)
 
top_book_user = (
    book_readed_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .book_id.values
)
 
book_df_rows = book_df[book_df['book_id'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.original_title)
print()
print('====' * 8)
print('Top 10 book recommendation')
print('----' * 8)
 
recommended_book = book_df[book_df['book_id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.original_title)

"""#### Recommendation for Book that you choose"""

what_book = "Peter and the Shadow Thieves"
book_readed = new_df[new_df.original_title.eq(what_book)]

book_not_readed = book_df[~book_df['book_id'].isin(book_readed.book_id.values)]['book_id'] 
book_not_readed = list(
    set(book_not_readed)
    .intersection(set(book_to_book_encoded.keys()))
)
 
book_not_readed = [[book_to_book_encoded.get(x)] for x in book_not_readed]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_readed), book_not_readed)
)

ratings_predict_2 = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings_predict_2.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_readed[x][0]) for x in top_ratings_indices
]
 
print('\mThe Book you read\n')
print('----' * 8)

print(what_book) 
 
book_df_rows = book_df[book_df['book_id'].isin(book_readed)]
for row in book_df_rows.itertuples():
    print(row.original_title)

print()
print('====' * 8)
print('Top 10 book recommendation')
print('----' * 8)
 
recommended_book = book_df[book_df['book_id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.original_title)